---
title: Publication
permalink: /publication/
---





<hr>

### 2022

[PREPRINT] [_Three simple steps to improve the interpretability of EEG-SVM studies_](https://www.biorxiv.org/content/10.1101/2021.12.14.472588v1)<br>
*Coralie Joucla*, Damien Gabriel, Juan-Pablo Ortega & Emmanuel Haffen<br>
BiorXiv 2021.12.14.472588, submitted, 2022 <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2022/Joucla_biorxiv_2022.pdf)

[_It’s not what you say, it’s how you say it: a retrospective study of the impact of prosody on own-name P300 in comatose patients_]()<br>
*Estelle Pruvost-Robieux*, Nathalie André-Obadia, Angela Marchi, Tarek Sharshar, Marco Liuni, Martine Gavaret & *Jean-Julien Aucouturier*<br>
Clinical Neurophysiology, in press, 2022 <br>

<hr>

### 2021

[_The shallow of your smile: the ethics of expressive vocal deep-fakes_](https://royalsocietypublishing.org/doi/10.1098/rstb.2020.0396) <br>
*Nadia Guerouaou*, Guillaume Vaiva & *JJ Aucouturier*<br>
Philosophical Transations of the Royal Society B, vol. 377 (1841), 2021 <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2021/Guerouaou_Philosophical_Transactions_2021.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/data.jpg'>](https://github.com/creamlab/deep-ethics)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/code.jpg'>](https://github.com/creamlab/deep-ethics)

[_Even violins can cry: specifically vocal emotional behaviours also drive the perception of emotions in non-vocal music_]()<br>
*Daniel Bedoya*, *Pablo Arias*, *Laura Rachman*, Marco Liuni, Clément Canonne, *Louise Goupil* & *JJ Aucouturier*<br>
Philosophical Transations of the Royal Society B, vol. 376(1840), 2021 <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2021/Bedoya_Philosophical_Transactions_2021.pdf)
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/code.jpg'>](https://github.com/creamlab/smiling_violins)

[_Facial mimicry in the congenitally blind_](https://www.cell.com/current-biology/fulltext/S0960-9822(21)01195-7)<br>
*Pablo Arias*, Caren Bellmann & *JJ Aucouturier*<br>
Current Biology, vol. 31(19), PR1112-R1114 (2021) <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2021/Arias_Current_Biology_2021.pdf)

[_Distinct signatures of subjective confidence and objective accuracy in speech prosody_](https://www.sciencedirect.com/science/article/abs/pii/S0010027721000809)<br>
*Louise Goupil* & *JJ Aucouturier*<br>
Cognition, vol. 212, 104661 (2021) <br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2021/Goupil_Cognition_2021.pdf)

[_Emergent Shared Intentions Support Coordination During Collective Musical Improvisations_](https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12932) <br>
*Louise Goupil*, Thomas Wolf, Pierre Saint-Germier, *JJ Aucouturier* & Clément Canonne<br>
Cognitive Science, vol. 45. (2021)<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2021/Goupil_Cognitive_Science_2021.pdf)

[_Vocal signals only impact speakers’ own emotions when they are self-attributed_](https://www.sciencedirect.com/science/article/abs/pii/S1053810020305390?dgcid=coauthor)<br>
*Louise Goupil*, Petter Johansson, Lars Hall & *JJ Aucouturier* <br>
Consciousness & Cognition, vol. 88, 103072 (2021)<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2021/Goupil_Consciousness_Cognition_2021.pdf)

[_Listeners perception of certainty and honesty of another speaker is associated with a common prosodic signature_](https://www.nature.com/articles/s41467-020-20649-4)<br>
*Louise Goupil*, *Emmanuel Ponsot*, Daniel Richardson, Gabriel Reyes & *JJ Aucouturier*<br>
Nature Communications, vol. 12, 861 (2021)<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2021/Goupil_Nature_Communications_2021.pdf)


<hr>

### 2016 - 2020

*Note:*  Articles published before 2020 correspond to work conducted in the [CREAM music neuroscience team](https://cream.ircam.fr) in IRCAM. We list here a selection of publications that are important to our current research. For a complete list of publications on music cognition and vocal emotions from the CREAM team (2016-2020), see the [CREAM archive page]({{site.baseurl}}/cream). For even earlier work on machine learning and audio signal processing, see JJA's [Google Scholar page](https://scholar.google.com/citations?user=jnST06UAAAAJ). 

#### Psychophysical reverse-correlation 

[_CLEESE: An open-source audio-transformation toolbox for data-driven experiments in speech and music cognition_](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0205943)<br>
*Juan Jose Burred*, *Emmanuel Ponsot*, *Louise Goupil*, *Marco Liuni* & *JJ Aucouturier*<br>
PLoS one, 14(4), e0205943, 2019<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2019/Burred_PLOS_One_2019.pdf)

[_Cracking the social code of speech prosody using reverse correlation_](https://www.pnas.org/content/115/15/3972)<br>
*Emmanuel Ponsot*, *Juan Jose Burred*, Pascal Belin & *JJ Aucouturier*<br>
Proceedings of the National Academy of Sciences, vol 115 (15) 3972-3977, 2018<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2018/Ponsot_PNAS_2018.pdf)

[_Uncovering mental representations of smiled speech using reverse correlation_](https://asa.scitation.org/doi/10.1121/1.5020989)<br>
*Emmanuel Ponsot*, *Pablo Arias* & *JJ Aucouturier*<br>
Journal of the Acoustical Society of America, vol 143 (1), 2018.<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2018/Ponsot_JASA_2018.pdf)

#### Vocal feedback 

[_DAVID: An open-source platform for real-time transformation of infra-segmental emotional cues in running speech_](https://link.springer.com/article/10.3758/s13428-017-0873-y)<br>
*Laura Rachman*, *Marco Liuni*, *Pablo Arias*, Andreas Lind, Petter Johansson, Lars Hall, Daniel Richardson, Katsumi Watanabe, Stéphanie Dubal & *JJ Aucouturier*<br>
Behaviour Research Methods, vol. 50(1), 323–343, 2017<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2017/Rachman_BRM_2017.pdf)

[_Covert Digital Manipulation of Vocal Emotion Alter Speakers’ Emotional State in a Congruent Direction_](https://www.pnas.org/content/113/4/948)<br>
*JJ Aucouturier*, Petter Johansson, Lars Hall, Rodrigo Segnini, Lolita Mercadié & Katsumi Watanabe<br>
Proceedings of the National Academy of Sciences, vol. 113 no. 4, 2016<br>
[<img style='display:inline;padding-top: 5px;' height='25' src='/images/access.jpg'>]({{site.baseurl}}/articles/2016/Aucouturier_PNAS_2016.pdf)


<hr>

### Copyright Notice

The documents listed here are available for downloading and have been provided as a means to ensure timely dissemination of scholarly and technical work on a noncommercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright. These works may not be re-posted without the explicit permission of the copyright holder.
